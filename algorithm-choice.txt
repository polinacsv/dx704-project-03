I decided to use Thompson Sampling with a Gaussian prior. Since the bandit rewards are stationary (the distribution doesn’t change over time), bounded (rewards stay within a fixed range), and not Bernoulli (not just 0/1 outcomes, but continuous values), I thought this method was a good fit. I also felt it was a better choice than ε-greedy, which explores randomly without using uncertainty, and UCB, which can be sensitive to reward scaling. Thompson Sampling seemed like the most balanced and practical option for this problem.
